{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwQU4L3F75h39eiyyzVZpb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlV5nypFalRj",
        "outputId": "d20b54ba-3c9e-41bc-c078-b2d91751e3a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.test.gpu_device_name())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A78JjLOOYokh",
        "outputId": "8596a2d1-b52d-4ad1-adfb-95495f323bdc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TFRobertaForSequenceClassification, RobertaTokenizer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "zCIL5ySlXUjk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure TensorFlow to use the GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
        "id": "wiM3ico6Xhv8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the concatenated dataset\n",
        "df = pd.read_csv('normydata.csv')"
      ],
      "metadata": {
        "id": "0sNJPuGmXmT_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "# train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['emotion'], random_state=42)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "val_df = pd.read_csv('val.csv')"
      ],
      "metadata": {
        "id": "cz-zOwTZXrUt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "train_df['emotion_encoded'] = le.fit_transform(train_df['emotion'])\n",
        "val_df['emotion_encoded'] = le.transform(val_df['emotion'])"
      ],
      "metadata": {
        "id": "gRamAK6Vic-w"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "zlhZ9jUdklP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_df['emotion_encoded']), y=train_df['emotion_encoded'])\n",
        "\n",
        "# Convert the class weights to a dictionary\n",
        "class_weights_dict = dict(enumerate(class_weights))\n"
      ],
      "metadata": {
        "id": "1WH_FRbkXxu5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained RoBERTa tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "model = TFRobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=8)"
      ],
      "metadata": {
        "id": "nByvQLWWX1Kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd106c5-8d06-4ebc-86de-aeb9932bf8ef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and loss function\n",
        "optimizer = Adam(learning_rate=1e-5)\n",
        "loss = SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "jItwxb8PX3Zx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Define the learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 3:\n",
        "        return 1e-5\n",
        "    else:\n",
        "        return 1e-6"
      ],
      "metadata": {
        "id": "C_u247M-X6S9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2)"
      ],
      "metadata": {
        "id": "bu2ZOUvCX-2h"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input data\n",
        "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True, max_length=128)"
      ],
      "metadata": {
        "id": "sJxmtxBlYBZJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_df['emotion_encoded'].tolist()\n",
        ")).shuffle(len(train_df)).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_df['emotion_encoded'].tolist()\n",
        ")).batch(batch_size)"
      ],
      "metadata": {
        "id": "L9WcRJsFYDt3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "with tf.device('/GPU:0'):\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  sensei = model.fit(train_dataset, \n",
        "                      validation_data=val_dataset, \n",
        "                      class_weight=class_weights_dict,\n",
        "                      epochs=epochs, \n",
        "                      callbacks=[early_stopping, LearningRateScheduler(lr_schedule)])"
      ],
      "metadata": {
        "id": "TQw5TZIyYGwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8c8060-d8a2-45ae-ae92-10d71d7aeedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 1318/10990 [==>...........................] - ETA: 3:23:08 - loss: 1.5952 - accuracy: 0.3926"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATION OF THE TEST VAL AND TRAIN SETS\n",
        "```\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('normydata.csv')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the train data into train and validation sets\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the train, validation, and test sets to CSV files\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "val_df.to_csv('val.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "0GikzoqzZqoj"
      }
    }
  ]
}